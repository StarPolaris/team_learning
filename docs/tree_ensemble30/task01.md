# Task01 决策树（上）

### 知识回顾

* 1.ID3树算法、C4.5树算法和CART算法之间有何异同？

    C4.5算法在ID3算法的基础上做出了诸多改进，包括但不限于：处理数值特征、处理含缺失值的特征、使用信息增益比代替信息增益以及给出树的剪枝策略；
    当处理分类问题时，虽然ID3或C4.5定义的熵仍然可以使用，但是由于对数函数log的计算代价较大，CART将熵中的log在p=1处利用一阶泰勒展开，基尼系数定义为熵的线性近似。

* 2.什么是信息增益？它衡量了什么指标？它有什么缺陷？

    信息增益：Information Gain，即节点分裂之后带来了多少不确定性的降低或纯度的提高。在得到了随机变量X的取值信息时，随机变量Y不确定性的平均减少量为$G(Y,X)=H(Y)−H(Y|X)$；
    衡量指标：节点分裂之后带来了多少不确定性的降低或纯度的提高；
    缺陷：对数函数log的计算代价较大；

* 3.sklearn决策树中的random_state参数控制了哪些步骤的随机性？

    max_feature数据特征的抽取，控制random_split产生均匀分布时产生随机性；

* 4.决策树如何处理连续变量和缺失变量？

    连续变量：最佳分割法和随机分割法划分样本集合；
    缺失变量：样本的缺失值占比越大，那么对信息增益的惩罚就越大。设节点N的样本缺失值比例为γ，记非缺失值对应的类别标签和特征分别为$\tilde{Y}$和$\tilde{X}$，则修正的信息增益为$\tilde{G}(Y,X)=(1−\gamma)G(\tilde{Y},\tilde{X})$；

* 5.基尼系数是什么？为什么要在CART中引入它？

    基尼系数：基尼系数定义为熵的线性近似，即由于$H(Y)=E_{Y}I(p)=E_{Y}[−log_{2}p(Y)]≈E_{Y}[1−p(Y)]$, 从而定义基尼系数为: $Gini(Y)=E_{Y}[1−p(Y)]=\sum_{k=1}^{K}\tilde{p}(y_{k})(1−\tilde{p}(y_{k}))=1−\sum_{k=1}^{K}\tilde{p}^{2}(y_{k})$

    引入理由：由于对数函数$log$的计算代价较大，CART将熵中的$log$在$p=1$处利用一阶泰勒展开。

* 6.什么是树的预剪枝和后剪枝？具体分别是如何操作的？

    预剪枝：预剪枝是指树在判断节点是否分裂的时候就预先通过一些规则来阻止其分裂；对每个结点划分前先进行估计，若当前结点的划分不能带来决策树的泛化性能的提升，则停止划分，并标记为叶结点。<br/>
    后剪枝：指在树的节点已经全部生长完成后，通过一些规则来摘除一些子树；后剪枝的依据为$E(Node^{N})=\frac{R(Node^{N})−R(T)}{|T^{N}|−1}\leqα$,在树完全生成后就会把所有节点的$E(Node^{N})$值进行记录，每次剪枝都会分别查看所有非叶子节点的树节点对应的$E(Node^{N})$值，并且对具有最小$E(Node^{N})$值的非叶子节点进行剪枝，直到所有节点的$E(Node^{N})$值都大于给定的cpp_alpha。

### 侧边习题

* 定义$X,Y$的联合熵为$H(Y,X)$为$E_{(Y,X)∼p(y,x)}[−\log_{2}p(Y,X)]$, 请证明如下关系：

    $ G(Y,X)=H(X)−H(X|Y) $

    解答：
    $H(X)−H(X|Y) = E_{x}[-\log_{2}p(X)] - E_{y}[E_{X|Y}(-\log_{2}p(X|Y))] $<br/>

    $= -\sum_{k=1}^{K}p(x_{k})\log_{2}p(x_{k}) + \sum_{m=1}^{M}p(y_{m})\sum_{k=1}^{K}p(x_{k}|Y=y_{m})\log_{2}p(x_{k}|Y=y_{m})) $<br/>

    $ = -\sum_{k=1}^{K}[\sum_{m=1}^{M}p(x_{k},y_{m})]\log_{2}p(x_{k}) + \sum_{k=1}^{K}\sum_{m=1}^{M}p(y_{m})\frac{p(x_{k},y_{m})}{p(y_{m})}\log_{2}\frac{p(x_{k},y_{m})}{p(y_{m})} $<br/>

    $ = \sum_{k=1}^{K}\sum_{m=1}^{M}p(y_{m},x_{k})[\log_{2}\frac{p(x_{k},y_{m})}{p(y_{m})}-\log_{2}p(x_k)]
    = G(Y,X)
    $

    $ G(Y,X)=H(X)+H(Y)−H(Y,X) $

    解答：
    $ H(X)+H(Y)−H(Y,X) = E_{x}[-\log_{2}p(X)] + E_{y}[-\log_{2}p(Y)] - E_{(Y,X)∼p(y,x)}[−\log_{2}p(Y,X)]$ <br/>

    $= -\sum_{k=1}^{K}p(x_{k})\log_{2}p(x_{k}) -\sum_{m=1}^{M}p(y_{m})\log_{2}p(y_{m}) + \sum_{k=1}^{K}\sum_{m=1}^{M}p(x_{k},y_{m})\log_{2}p(x_{k},y_{m}) $

    $= -\sum_{k=1}^{K}[\sum_{m=1}^{M}p(x_{k},y_{m})]\log_{2}p(x_{k}) -\sum_{m=1}^{M}[\sum_{k=1}^{K}p(x_{k},y_{m})]\log_{2}p(y_{m}) + \sum_{k=1}^{K}\sum_{m=1}^{M}p(x_{k},y_{m})\log_{2}p(x_{k},y_{m}) $

    $= \sum_{k=1}^{K}\sum_{m=1}^{M}p(y_{m},x_{k})[-log_{2}p(x_{k})-log_{2}p(y_{m})+log_{2}p(x_{k},y_{m})] = G(Y,X)$

    $ G(Y,X)=H(Y,X)−H(X|Y)−H(Y|X) $

    解答：
    根据以上式子，由$$2*G(Y,X)=H(X)−H(X|Y) + H(Y)-H(Y|X)$$
    $$ G(Y,X)=H(X)+H(Y)−H(Y,X) $$
    两式左右相减得到$G(Y,X)=H(Y,X)−H(X|Y)−H(Y|X)$


* 下图被分为了$A、B$和$C$三个区域。若$AB$区域代表$X$的不确定性，$BC$区域代表$Y$的不确定性，那么$H(X)、H(Y)、H(X|Y)、H(Y|X)、H(Y,X)$和$G(Y,X)$分别指代的是哪片区域？

    解答：$H(X): AB$ &nbsp; $H(Y): BC$  &nbsp; $H(X|Y): A$ &nbsp; $H(Y|X): C$ &nbsp; $H(Y,X): ABC$ &nbsp; $G(Y|X): B$<br/>

* 假设当前我们需要处理一个分类问题，请问对输入特征进行归一化会对树模型的类别输出产生影响吗？请解释原因。

    解答：不会，因为数据集分类的相对位置并没有改变，$H(Y|X)$与$H(Y)$并不会改变，因而$G(Y,X)$不变。

* 如果将系数替换为$1 - \gamma^{2}$，请问对缺失值是加强了还是削弱了惩罚？

    解答：$1-\gamma \leq 1 - \gamma^{2}$, 削弱了惩罚。

* 如果将树的生长策略从深度优先生长改为广度优先生长，假设其他参数保持不变的情况下，两个模型对应的结果输出可能不同吗？

    解答：不可能，因为其他参数不变的时候，树的模型与分裂的顺序无关。

* 在一般的机器学习问题中，我们总是通过一组参数来定义模型的损失函数，并且在训练集上以最小化该损失函数为目标进行优化。请问对于决策树而言，模型优化的目标是什么？

    解答：优化的目标为树的剪枝度量，直到所有节点的$E(Node^{N})$值都大于给定的cpp_alpha。

* 对信息熵中的log函数在p=1处进行一阶泰勒展开可以近似为基尼系数，那么如果在p=1处进行二阶泰勒展开我们可以获得什么近似指标？请写出对应指标的信息增益公式。

    解答：二阶泰勒展开：$\log_{2}p = f(1) + \frac{f'(1)}{1!}(p-1) + \frac{f''(1)}{2!}(p-1)^{2} = \frac{3}{2} - 2p + \frac{1}{2}p^{2}$

    $Gini(Y)= \sum_{k=1}^{K}p(y_{k})(\frac{3}{2} - 2p(y_{k}) + \frac{1}{2}p^{2}(y_{k}))=\frac{3}{2}-2\sum_{k=1}^{K}p^{2}(y_{k})+\frac{1}{2}\sum_{k=1}^{K}p^{3}(y_{k})$

    $Gini(Y|X)=\sum_{m=1}^{M}p(x_{m})\sum_{k=1}^{K}[p(y_{k}|x_{m})(\frac{3}{2} - 2p(y_{k}|x_{m}) + \frac{1}{2}p(y_{k}|x_{m})^{2})]$<br/>

    $=\sum_{m=1}^{M}p(x_{m})[\frac{3}{2}-2\sum_{k=1}^{K}p^{2}(y_{k}|x_{m})+\frac{1}{2}\sum_{k=1}^{K}p^{3}(y_{k}|x_{m})]$

    $G(Y,X) = Gini(Y) - Gini(Y|X) = -2\sum_{k=1}^{K}p^{2}(y_{k})+\frac{1}{2}\sum_{k=1}^{K}p^{3}(y_{k}) + \sum_{m=1}^{M}p(x_{m})[2\sum_{k=1}^{K}p^{2}(y_{k}|x_{m})-\frac{1}{2}\sum_{k=1}^{K}p^{3}(y_{k}|x_{m})]$

* 除了信息熵和基尼系数之外，我们还可以使用节点的$1−max_{k}p(Y=y_{k})$和第$m$个子节点的$1−max_{k}p(Y=y_{k}|X=x_{m})$来作为衡量纯度的指标。请解释其合理性并给出相应的信息增益公式。

    解答：合理性：在单点分布和均匀分布的情况下均满足条件

    公式：$H(Y)=\sum_{k=1}^{K}p(y_{k})[1-max_{k}p(Y=y_{k})]=1-max_{k}p(Y=y_{k})$

    $H(Y|X)=\sum_{m=1}^{M}p(x_{m})\sum_{k=1}^{K}p(y_{k}|x_{m})[1-max_{k}p(Y=y_{k}|X=x_{m})]=1-max_{k}p(Y=y_{k}|X=x_{m})$

    $G(Y,X) = max_{k}p(Y=y_{k}|X=x_{m})-max_{k}p(Y=y_{k})$

* 为什么对没有重复特征值的数据，决策树能够做到损失为0？

    解答：数据没有重复特征值，意味着训练集数据可以在树的结构上无限分裂至每个子叶仅包含一个数据样本点，所以能够做到损失为0.

* 如何理解min_samples_leaf参数能够控制回归树输出值的平滑程度？

    解答：样本较小时，该参数决定了输出值的均值估计的方差大小，方差大则回归树输出值不够平滑。